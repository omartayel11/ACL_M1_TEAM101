# ğŸ¨ Hotel Travel Assistant Chatbot

## LangGraph-Native Conversational AI with Memory

Full-featured chatbot using LangGraph's native memory system (`MemorySaver`) for persistent conversation state across multiple turns.

---

## âœ… **What's Implemented**

### Core Features:
- âœ… **LangGraph Native Memory** - Uses `MemorySaver` with thread-based checkpointing
- âœ… **4 Workflow Modes** - baseline_only, embedding_only, hybrid, llm_pipeline
- âœ… **Query Rewriting** - Resolves references like "there", "that hotel" using conversation context
- âœ… **Conversation History** - Maintains full chat history with metadata
- âœ… **Thread Management** - Each conversation has a unique thread_id
- âœ… **Session Export/Import** - Save and restore conversations
- âœ… **Context-Aware Responses** - LLM sees recent conversation history

### Components:
1. **`chatbot.py`** - Main chatbot class with LangGraph memory
2. **`components/conversation_manager.py`** - Conversation history management
3. **`components/query_rewriter.py`** - Resolve references using LLM
4. **`workflows/workflow_factory.py`** - Workflow selection with memory support

---

## ğŸš€ **Quick Start**

### Interactive Chat (CLI):
```bash
python chatbot.py
```

### Programmatic Usage:
```python
from chatbot import HotelChatbot

# Create chatbot with workflow
bot = HotelChatbot(workflow_mode="hybrid")

# Chat
response = bot.chat("Find luxury hotels in Paris")
print(response['answer'])

# Follow-up (automatic context resolution)
response = bot.chat("What about 5-star hotels there?")
# "there" automatically resolves to "Paris"
print(response['answer'])

# Check history
history = bot.get_conversation_history()
print(f"Messages: {len(history)}")

# Export session
session_data = bot.export_session()

# Clear and start fresh
bot.clear_history()
```

---

## ğŸ“Š **Workflow Modes**

| Workflow | Description | Speed | Quality | Use Case |
|----------|-------------|-------|---------|----------|
| `baseline_only` | Intent â†’ Entity â†’ Cypher | âš¡ Fast | Good | Structured queries |
| `embedding_only` | Semantic vector search | âš¡ Fast | Better | Natural language |
| `hybrid` | Parallel baseline + embedding | ğŸ¢ Slow | **Best** | Maximum accuracy |
| `llm_pipeline` | LLM generates Cypher | ğŸ¢ Slowest | Variable | Full autonomy |

---

## ğŸ’¬ **Query Rewriting Examples**

The chatbot automatically resolves references using conversation context:

```python
bot.chat("Find hotels in Tokyo")
# Next query:
bot.chat("What about luxury hotels there?")
# Rewrites to: "What about luxury hotels in Tokyo?"

bot.chat("Show me The Ritz Paris")
# Next query:
bot.chat("What are the reviews for that hotel?")
# Rewrites to: "What are the reviews for The Ritz Paris?"
```

---

## ğŸ”§ **Advanced Features**

### Switch Workflows Mid-Conversation:
```python
bot = HotelChatbot(workflow_mode="embedding_only")
bot.chat("Find hotels in Rome")

# Switch to hybrid for better results
bot.switch_workflow("hybrid")
bot.chat("Show me 5-star hotels there")
```

### Export/Import Sessions:
```python
# Export
session = bot.export_session()
with open("session.json", "w") as f:
    json.dump(session, f)

# Import later
with open("session.json", "r") as f:
    session = json.load(f)
bot.import_session(session)
```

### Get LangGraph Thread State:
```python
state = bot.get_thread_state()
print(f"Thread ID: {bot.thread_id}")
print(f"Messages: {len(bot.message_history)}")
```

---

## ğŸ§ª **Testing**

### Run Tests:
```bash
# Test chatbot functionality
python test_chatbot.py

# Test individual components
python components/conversation_manager.py
python components/query_rewriter.py

# Test workflows directly
python test_embedding_workflow.py
python -m workflows.baseline_workflow
```

---

## ğŸ—ï¸ **Architecture**

```
User Query
    â†“
Query Rewriter (resolve "it", "there", etc.)
    â†“
LangGraph Workflow (with MemorySaver)
    â†“
    â”œâ”€ Intent Classification
    â”œâ”€ Entity Extraction
    â”œâ”€ Cypher Query / Vector Search
    â”œâ”€ Result Merging
    â””â”€ LLM Answer Generation (with chat history)
    â†“
Response + Update History
    â†“
Store in MemorySaver (thread_id)
```

---

## ğŸ“ **Configuration**

Edit `config.yaml`:
```yaml
retrieval:
  embedding:
    similarity_threshold: 0.3  # Lower = more results
    max_results: 10

llm:
  temperature: 0.7
  max_tokens: 500
```

---

## ğŸ”® **Next Steps**

1. âœ… **Chatbot with LangGraph Memory** - âœ… DONE
2. â³ **Streamlit UI** - Use chatbot in web interface
3. â³ **Evaluation Framework** - Compare workflow performance

---

## ğŸ“š **References**

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **Memory/Persistence**: https://langchain-ai.github.io/langgraph/how-tos/persistence/
- **Checkpointing**: https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/
